{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install necessary libraries\n",
    "!pip install monai pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.transforms import LoadImage  # Monai for DICOM loading\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# TPU Setup\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.TPUStrategy(resolver)\n",
    "\n",
    "\n",
    "# Step 1: Function to load DICOM images using monai\n",
    "def load_dicom_images(image_path, img_size):\n",
    "    load_image = LoadImage(image_only=True)\n",
    "    img = load_image(image_path)\n",
    "    \n",
    "    # Check if the image has 2 dimensions (grayscale) and expand it to 3 dimensions\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.expand_dims(img, axis=-1)  # Add a channel dimension (grayscale to (H, W, 1))\n",
    "    \n",
    "    # Resize the image to the target size\n",
    "    img_resized = tf.image.resize(img, img_size)\n",
    "    \n",
    "    # If the image is grayscale, convert it to 3 channels (RGB-like)\n",
    "    if img_resized.shape[-1] == 1:  # Grayscale\n",
    "        img_resized = tf.image.grayscale_to_rgb(img_resized)\n",
    "    \n",
    "    img_resized = img_resized / 255.0  # Normalize pixel values\n",
    "    return img_resized.numpy()\n",
    "\n",
    "# Step 2: Data Generator for DICOM images\n",
    "class DICOMDataGenerator(Sequence):\n",
    "    def __init__(self, df, batch_size, img_size, augment=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        self.indices = np.arange(len(self.df))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_indices = self.indices[index * self.batch_size: (index + 1) * self.batch_size]\n",
    "        batch_df = self.df.loc[batch_indices]\n",
    "\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for i, row in batch_df.iterrows():\n",
    "            dicom_path = row['image_path']\n",
    "            try:\n",
    "                img = load_dicom_images(dicom_path, self.img_size)\n",
    "                images.append(img)\n",
    "                labels.append(row['cancer'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {dicom_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        images = np.array(images)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        if self.augment and len(images) > 0:\n",
    "            aug_gen = ImageDataGenerator(rotation_range=20,\n",
    "                                         width_shift_range=0.1,\n",
    "                                         height_shift_range=0.1,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True)\n",
    "            images = aug_gen.flow(images, shuffle=False, batch_size=len(images)).__getitem__(0)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Step 3: Load CSV and update file paths\n",
    "df = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/train.csv')\n",
    "df['image_path'] = df.apply(lambda row: f\"/kaggle/input/rsna-breast-cancer-detection/train_images/{row['patient_id']}/{row['image_id']}.dcm\", axis=1)\n",
    "\n",
    "# Step 4: Split data into training and validation sets\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df['cancer'], random_state=42)\n",
    "\n",
    "# Image size and batch size\n",
    "img_size = (224, 224)\n",
    "batch_size = 8  # Reduced batch size to manage memory usage\n",
    "\n",
    "# Step 5: Create data generators for training and validation\n",
    "train_gen = DICOMDataGenerator(train_df, batch_size=batch_size, img_size=img_size, augment=True)\n",
    "valid_gen = DICOMDataGenerator(valid_df, batch_size=batch_size, img_size=img_size, augment=False)\n",
    "\n",
    "# Use TPU strategy for model training\n",
    "with strategy.scope():\n",
    "    # Step 6: Build the model (Transfer learning with ResNet50)\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(*img_size, 3))\n",
    "    base_model.trainable = False  # Freeze base model layers\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        base_model,\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    # Step 7: Save the best model during training\n",
    "    checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    # Step 8: Train the model with early stopping and checkpointing\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(train_gen, \n",
    "                        validation_data=valid_gen, \n",
    "                        epochs=10,  # Increased epochs\n",
    "                        callbacks=[early_stopping, checkpoint])\n",
    "\n",
    "# Step 9: Plot training and validation accuracy and loss\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Make predictions for submission\n",
    "test_dir = '/kaggle/input/rsna-breast-cancer-detection/test_images'\n",
    "\n",
    "# Load test data from CSV and create file paths for test images\n",
    "test_df = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\n",
    "test_df['image_path'] = test_df.apply(lambda row: f\"{test_dir}/{row['patient_id']}/{row['image_id']}.dcm\", axis=1)\n",
    "\n",
    "# Create a test data generator\n",
    "test_gen = DICOMDataGenerator(test_df, batch_size=1, img_size=img_size, augment=False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(test_gen)\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'prediction_id': test_df['prediction_id'],\n",
    "    'cancer': predictions.flatten()\n",
    "})\n",
    "\n",
    "# Clip predictions between 0 and 1\n",
    "submission['cancer'] = submission['cancer'].apply(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
